{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1sUfcA8ZgR2t"
      },
      "source": [
        "# Train YOLO Model in Google Colab\n",
        "\n",
        "**GitHub:** [Train YOLO Model](https://github.com/Etaizil/Train_Yolo_Model)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3NW7LLv_QPOO"
      },
      "source": [
        "**Verify NVIDIA GPU Availability**\n",
        "\n",
        "Make sure you're using a GPU-equipped machine by going to \"Runtime\" -> \"Change runtime type\" in the top menu bar, and then selecting one of the GPU options in the Hardware accelerator section. Click Play on the following code block to verify that the NVIDIA GPU is present and ready for training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cfaWho47RGDf",
        "outputId": "b1f474e4-ab98-4914-b7ae-fa9e386453e0"
      },
      "outputs": [],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pub8jU44eWb2"
      },
      "source": [
        "# Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b2zqXYbreX6z"
      },
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "import os\n",
        "import sys\n",
        "import random\n",
        "import shutil"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8eDhuvzDfIFS"
      },
      "source": [
        "# Upload Image Dataset and Prepare Training Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZW_0c110fOiz"
      },
      "source": [
        "Next, we'll upload our dataset and prepare it for training with YOLO. We'll split the dataset into train and validation folders, and we'll automatically generate the configuration file for training the model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FwKAqFIQSBpn"
      },
      "source": [
        "## Upload images\n",
        "\n",
        "First, we need to upload the dataset to Colab. Here are a few options for moving the `data.zip` folder into this Colab instance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hAjn8GvZ7QQ8",
        "outputId": "b6db6b29-6d73-43d3-cb5d-4d1e59de7a7d"
      },
      "outputs": [],
      "source": [
        "!gdown --id ID_OF_DATA -O /content/data.zip"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m7Iz9eBzW5zm"
      },
      "source": [
        "## Split images into train and validation folders"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "58JuFGc2PatU"
      },
      "source": [
        " Click the folder icon on the left and see your `data.zip` file in the list of files.\n",
        " \n",
        " Next, we'll unzip `data.zip` and create some folders to hold the images.\n",
        " \n",
        " Run the following code block to unzip the data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "z8O6z-wVcPEF"
      },
      "outputs": [],
      "source": [
        "# Unzip images to a custom data folder\n",
        "!unzip -q /content/data.zip -d /content/custom_data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eoPjqW6AYebn"
      },
      "source": [
        "Ultralytics requires a particular folder structure to store training data for models. The root folder is named ‚Äúdata‚Äù. Inside, there are two main folders:\n",
        "\n",
        "*   **Train**: These are the actual images used to train the model. In one epoch of training, every image in the train set is passed into the neural network. The training algorithm adjusts the network weights to fit the data in the images.\n",
        "\n",
        "\n",
        "*   **Validation**: These images are used to check the model's performance at the end of each training epoch."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WGCJsJCQ8T2w",
        "outputId": "fa76b42e-4ace-44f8-cbe0-9194f14582e0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Created folder at /content/data/train/images.\n",
            "Created folder at /content/data/train/labels.\n",
            "Created folder at /content/data/validation/images.\n",
            "Created folder at /content/data/validation/labels.\n",
            "Number of image files: 1187\n",
            "Number of annotation files: 1187\n",
            "Images moving to train: 949\n",
            "Images moving to validation: 238\n"
          ]
        }
      ],
      "source": [
        "data_path = \"/content/custom_data\"\n",
        "train_percent = 0.8\n",
        "\n",
        "if not os.path.isdir(data_path):\n",
        "    print(f'Directory {data_path} not found. Verify the path and try again.')\n",
        "    sys.exit(0)\n",
        "\n",
        "if train_percent < 0.01 or train_percent > 0.99:\n",
        "    print('Invalid entry for train_pct. Please enter a number between 0.01 and 0.99.')\n",
        "    sys.exit(0)\n",
        "\n",
        "val_percent = 1 - train_percent\n",
        "\n",
        "input_image_path = os.path.join(data_path, 'images')\n",
        "input_label_path = os.path.join(data_path, 'labels')\n",
        "\n",
        "cwd = os.getcwd()\n",
        "train_img_path = os.path.join(cwd, 'data/train/images')\n",
        "train_txt_path = os.path.join(cwd, 'data/train/labels')\n",
        "val_img_path = os.path.join(cwd, 'data/validation/images')\n",
        "val_txt_path = os.path.join(cwd, 'data/validation/labels')\n",
        "\n",
        "for dir_path in [train_img_path, train_txt_path, val_img_path, val_txt_path]:\n",
        "    if not os.path.exists(dir_path):\n",
        "        os.makedirs(dir_path)\n",
        "        print(f'Created folder at {dir_path}.')\n",
        "\n",
        "img_file_list = list(Path(input_image_path).rglob('*'))\n",
        "txt_file_list = list(Path(input_label_path).rglob('*'))\n",
        "\n",
        "print(f'Number of image files: {len(img_file_list)}')\n",
        "print(f'Number of annotation files: {len(txt_file_list)}')\n",
        "\n",
        "file_num = len(img_file_list)\n",
        "train_num = int(file_num * train_percent)\n",
        "val_num = file_num - train_num\n",
        "print(f'Images moving to train: {train_num}')\n",
        "print(f'Images moving to validation: {val_num}')\n",
        "\n",
        "random.shuffle(img_file_list)\n",
        "\n",
        "for i, set_num in enumerate([train_num, val_num]):\n",
        "    for _ in range(set_num):\n",
        "        if not img_file_list:\n",
        "            break\n",
        "\n",
        "        img_path = img_file_list.pop()\n",
        "        img_fn = img_path.name\n",
        "        base_fn = img_path.stem\n",
        "        txt_fn = base_fn + '.txt'\n",
        "        txt_path = os.path.join(input_label_path, txt_fn)\n",
        "\n",
        "        if i == 0:\n",
        "            new_img_path, new_txt_path = train_img_path, train_txt_path\n",
        "        else:\n",
        "            new_img_path, new_txt_path = val_img_path, val_txt_path\n",
        "\n",
        "        shutil.copy(img_path, os.path.join(new_img_path, img_fn))\n",
        "\n",
        "        if os.path.exists(txt_path):\n",
        "            shutil.copy(txt_path, os.path.join(new_txt_path, txt_fn))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B2L2qGCJzwY9"
      },
      "source": [
        "# Install Requirements (Ultralytics)\n",
        "\n",
        "Next, we'll install the Ultralytics library in this Google Colab instance. This Python library will be used to train the YOLO model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EMEDk5byzxY5",
        "outputId": "fa2ffeaf-415a-409c-edc9-a701d03a61cc"
      },
      "outputs": [],
      "source": [
        "!pip install ultralytics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cuZoMkSFN9XG"
      },
      "source": [
        "# Configure Training\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0c5Kdh0GmQHS"
      },
      "source": [
        "There's one last step before we can run training: we need to create the Ultralytics training configuration YAML file. This file specifies the location of your train and validation data, and it also defines the model's classes.\n",
        "\n",
        "Run the code block below to generate a `data.yaml` configuration file. Make sure you have a labelmap file located at `custom_data/classes.txt`. If you used Label Studio, it should already be present. If you assembled the dataset another way, you may have to manually create the `classes.txt` file."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "4letvP7X12ji"
      },
      "outputs": [],
      "source": [
        "import yaml\n",
        "import os\n",
        "\n",
        "def create_data_yaml(path_to_classes_txt, path_to_data_yaml):\n",
        "\n",
        "  if not os.path.exists(path_to_classes_txt):\n",
        "    print(f'classes.txt file not found! Please create a classes.txt labelmap and move it to {path_to_classes_txt}')\n",
        "    return\n",
        "  with open(path_to_classes_txt, 'r') as f:\n",
        "    classes = []\n",
        "    for line in f.readlines():\n",
        "      if len(line.strip()) == 0: continue\n",
        "      classes.append(line.strip())\n",
        "  number_of_classes = len(classes)\n",
        "\n",
        "  data = {\n",
        "      'path': '/content/data',\n",
        "      'train': 'train/images',\n",
        "      'val': 'validation/images',\n",
        "      'nc': number_of_classes,\n",
        "      'names': classes\n",
        "  }\n",
        "\n",
        "  with open(path_to_data_yaml, 'w') as f:\n",
        "    yaml.dump(data, f, sort_keys=False)\n",
        "  print(f'Created config file at {path_to_data_yaml}')\n",
        "\n",
        "  return"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "50m6wSVwdeN0",
        "outputId": "391cfacf-92fb-45ec-9ac2-419968e1bdab"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Created config file at /content/data.yaml\n",
            "\n",
            "File contents:\n",
            "\n",
            "path: /content/data\n",
            "train: train/images\n",
            "val: validation/images\n",
            "nc: 3\n",
            "names:\n",
            "- Happy\n",
            "- Natural\n",
            "- Surprised\n"
          ]
        }
      ],
      "source": [
        "path_to_classes_txt = '/content/custom_data/classes.txt'\n",
        "path_to_data_yaml = '/content/data.yaml'\n",
        "\n",
        "create_data_yaml(path_to_classes_txt, path_to_data_yaml)\n",
        "\n",
        "print('\\nFile contents:\\n')\n",
        "!cat /content/data.yaml"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V17UjYU5ZQdR"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8bbpob1gTPlo",
        "outputId": "db397d42-4e8a-4547-a3c4-99dd36a64c9b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Ultralytics 8.3.88 üöÄ Python-3.11.11 torch-2.5.1+cu124 CUDA:0 (Tesla T4, 15095MiB)\n",
            "\u001b[34m\u001b[1mengine/trainer: \u001b[0mtask=detect, mode=train, model=yolo11m.pt, data=/content/data.yaml, epochs=30, time=None, patience=100, batch=16, imgsz=640, save=True, save_period=-1, cache=False, device=None, workers=8, project=None, name=train2, exist_ok=False, pretrained=True, optimizer=auto, verbose=True, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, freeze=None, multi_scale=False, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, vid_stride=1, stream_buffer=False, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, embed=None, show=False, save_frames=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, show_boxes=True, line_width=None, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=True, opset=None, workspace=None, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, bgr=0.0, mosaic=1.0, mixup=0.0, copy_paste=0.0, copy_paste_mode=flip, auto_augment=randaugment, erasing=0.4, crop_fraction=1.0, cfg=None, tracker=botsort.yaml, save_dir=runs/detect/train2\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1741785952.208708    4280 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1741785952.215090    4280 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "Overriding model.yaml nc=80 with nc=3\n",
            "\n",
            "                   from  n    params  module                                       arguments                     \n",
            "  0                  -1  1      1856  ultralytics.nn.modules.conv.Conv             [3, 64, 3, 2]                 \n",
            "  1                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n",
            "  2                  -1  1    111872  ultralytics.nn.modules.block.C3k2            [128, 256, 1, True, 0.25]     \n",
            "  3                  -1  1    590336  ultralytics.nn.modules.conv.Conv             [256, 256, 3, 2]              \n",
            "  4                  -1  1    444928  ultralytics.nn.modules.block.C3k2            [256, 512, 1, True, 0.25]     \n",
            "  5                  -1  1   2360320  ultralytics.nn.modules.conv.Conv             [512, 512, 3, 2]              \n",
            "  6                  -1  1   1380352  ultralytics.nn.modules.block.C3k2            [512, 512, 1, True]           \n",
            "  7                  -1  1   2360320  ultralytics.nn.modules.conv.Conv             [512, 512, 3, 2]              \n",
            "  8                  -1  1   1380352  ultralytics.nn.modules.block.C3k2            [512, 512, 1, True]           \n",
            "  9                  -1  1    656896  ultralytics.nn.modules.block.SPPF            [512, 512, 5]                 \n",
            " 10                  -1  1    990976  ultralytics.nn.modules.block.C2PSA           [512, 512, 1]                 \n",
            " 11                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
            " 12             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 13                  -1  1   1642496  ultralytics.nn.modules.block.C3k2            [1024, 512, 1, True]          \n",
            " 14                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
            " 15             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 16                  -1  1    542720  ultralytics.nn.modules.block.C3k2            [1024, 256, 1, True]          \n",
            " 17                  -1  1    590336  ultralytics.nn.modules.conv.Conv             [256, 256, 3, 2]              \n",
            " 18            [-1, 13]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 19                  -1  1   1511424  ultralytics.nn.modules.block.C3k2            [768, 512, 1, True]           \n",
            " 20                  -1  1   2360320  ultralytics.nn.modules.conv.Conv             [512, 512, 3, 2]              \n",
            " 21            [-1, 10]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 22                  -1  1   1642496  ultralytics.nn.modules.block.C3k2            [1024, 512, 1, True]          \n",
            " 23        [16, 19, 22]  1   1413337  ultralytics.nn.modules.head.Detect           [3, [256, 512, 512]]          \n",
            "YOLO11m summary: 231 layers, 20,055,321 parameters, 20,055,305 gradients, 68.2 GFLOPs\n",
            "\n",
            "Transferred 643/649 items from pretrained weights\n",
            "\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir runs/detect/train2', view at http://localhost:6006/\n",
            "Freezing layer 'model.23.dfl.conv.weight'\n",
            "\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks...\n",
            "\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed ‚úÖ\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mScanning /content/data/train/labels.cache... 949 images, 0 backgrounds, 0 corrupt: 100% 949/949 [00:00<?, ?it/s]\n",
            "\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01, num_output_channels=3, method='weighted_average'), CLAHE(p=0.01, clip_limit=(1.0, 4.0), tile_grid_size=(8, 8))\n",
            "\u001b[34m\u001b[1mval: \u001b[0mScanning /content/data/validation/labels.cache... 238 images, 0 backgrounds, 0 corrupt: 100% 238/238 [00:00<?, ?it/s]\n",
            "Plotting labels to runs/detect/train2/labels.jpg... \n",
            "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
            "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.001429, momentum=0.9) with parameter groups 106 weight(decay=0.0), 113 weight(decay=0.0005), 112 bias(decay=0.0)\n",
            "\u001b[34m\u001b[1mTensorBoard: \u001b[0mmodel graph visualization added ‚úÖ\n",
            "Image sizes 640 train, 640 val\n",
            "Using 2 dataloader workers\n",
            "Logging results to \u001b[1mruns/detect/train2\u001b[0m\n",
            "Starting training for 30 epochs...\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "       1/30      8.19G      1.066      1.897      1.404         10        640: 100% 60/60 [00:39<00:00,  1.50it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 8/8 [00:02<00:00,  2.85it/s]\n",
            "                   all        238        239      0.358      0.687      0.426      0.263\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "       2/30      8.19G      1.094      1.131      1.437         12        640: 100% 60/60 [00:37<00:00,  1.60it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 8/8 [00:02<00:00,  3.07it/s]\n",
            "                   all        238        239     0.0245     0.0698     0.0133    0.00338\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "       3/30      8.19G      1.099     0.9651      1.444         15        640: 100% 60/60 [00:37<00:00,  1.61it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 8/8 [00:02<00:00,  2.98it/s]\n",
            "                   all        238        239    0.00717      0.302    0.00486    0.00304\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "       4/30      8.19G      1.054     0.8195      1.417          8        640: 100% 60/60 [00:37<00:00,  1.62it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 8/8 [00:02<00:00,  3.06it/s]\n",
            "                   all        238        239      0.635      0.162      0.166       0.11\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "       5/30      8.19G      1.027     0.7946      1.397          8        640: 100% 60/60 [00:37<00:00,  1.60it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 8/8 [00:02<00:00,  2.98it/s]\n",
            "                   all        238        239      0.827      0.653      0.839      0.601\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "       6/30      8.19G      1.001     0.7689      1.391          5        640: 100% 60/60 [00:37<00:00,  1.60it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 8/8 [00:02<00:00,  2.93it/s]\n",
            "                   all        238        239       0.81      0.819       0.91      0.645\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "       7/30      8.19G      1.002      0.717      1.388         10        640: 100% 60/60 [00:36<00:00,  1.63it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 8/8 [00:02<00:00,  2.94it/s]\n",
            "                   all        238        239       0.95      0.847      0.953      0.647\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "       8/30      8.19G     0.9704     0.7172      1.349          8        640: 100% 60/60 [00:37<00:00,  1.61it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 8/8 [00:02<00:00,  2.99it/s]\n",
            "                   all        238        239      0.881      0.888      0.938      0.683\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "       9/30      8.19G      0.958     0.6721      1.349         13        640: 100% 60/60 [00:36<00:00,  1.63it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 8/8 [00:02<00:00,  3.00it/s]\n",
            "                   all        238        239      0.945      0.916      0.945      0.692\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "      10/30      8.19G     0.9222     0.6122      1.294         10        640: 100% 60/60 [00:37<00:00,  1.62it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 8/8 [00:02<00:00,  3.01it/s]\n",
            "                   all        238        239      0.961      0.959      0.978      0.703\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "      11/30      8.19G     0.9132     0.6043      1.295         12        640: 100% 60/60 [00:37<00:00,  1.62it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 8/8 [00:02<00:00,  2.99it/s]\n",
            "                   all        238        239      0.927      0.953      0.951      0.684\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "      12/30      8.19G     0.9115     0.5929      1.299          8        640: 100% 60/60 [00:36<00:00,  1.64it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 8/8 [00:02<00:00,  3.04it/s]\n",
            "                   all        238        239      0.929      0.861      0.934       0.68\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "      13/30      8.19G     0.9038     0.5952      1.291          8        640: 100% 60/60 [00:36<00:00,  1.63it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 8/8 [00:03<00:00,  2.48it/s]\n",
            "                   all        238        239      0.979      0.978      0.978      0.687\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "      14/30      8.19G     0.8673     0.5651      1.273         10        640: 100% 60/60 [00:36<00:00,  1.63it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 8/8 [00:03<00:00,  2.22it/s]\n",
            "                   all        238        239      0.953      0.965      0.972      0.722\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "      15/30      8.19G     0.8949     0.5576      1.295         13        640: 100% 60/60 [00:36<00:00,  1.63it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 8/8 [00:02<00:00,  3.00it/s]\n",
            "                   all        238        239      0.972      0.981      0.989      0.713\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "      16/30      8.19G     0.8619     0.5484      1.271         11        640: 100% 60/60 [00:36<00:00,  1.63it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 8/8 [00:02<00:00,  3.06it/s]\n",
            "                   all        238        239      0.984      0.965      0.989      0.743\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "      17/30      8.19G     0.8752     0.5408       1.25         12        640: 100% 60/60 [00:37<00:00,  1.62it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 8/8 [00:02<00:00,  2.73it/s]\n",
            "                   all        238        239      0.972      0.868      0.952      0.708\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "      18/30      8.19G      0.844     0.5571      1.249         10        640: 100% 60/60 [00:36<00:00,  1.64it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 8/8 [00:03<00:00,  2.34it/s]\n",
            "                   all        238        239      0.944      0.963      0.981      0.729\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "      19/30      8.19G     0.8551     0.5328      1.252         11        640: 100% 60/60 [00:37<00:00,  1.62it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 8/8 [00:02<00:00,  2.90it/s]\n",
            "                   all        238        239      0.972      0.982       0.99      0.735\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "      20/30      8.19G     0.8504     0.5271      1.255         14        640: 100% 60/60 [00:36<00:00,  1.63it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 8/8 [00:02<00:00,  2.98it/s]\n",
            "                   all        238        239      0.991       0.97      0.991      0.729\n",
            "Closing dataloader mosaic\n",
            "\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01, num_output_channels=3, method='weighted_average'), CLAHE(p=0.01, clip_limit=(1.0, 4.0), tile_grid_size=(8, 8))\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "      21/30      8.19G     0.8017     0.4216      1.301          5        640: 100% 60/60 [00:37<00:00,  1.60it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 8/8 [00:02<00:00,  3.01it/s]\n",
            "                   all        238        239       0.97      0.959      0.987      0.761\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "      22/30      8.19G     0.8035      0.416      1.305          5        640: 100% 60/60 [00:36<00:00,  1.65it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 8/8 [00:02<00:00,  3.03it/s]\n",
            "                   all        238        239      0.992      0.971      0.989      0.752\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "      23/30      8.19G     0.7964     0.3959      1.298          5        640: 100% 60/60 [00:36<00:00,  1.63it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 8/8 [00:02<00:00,  3.01it/s]\n",
            "                   all        238        239      0.975      0.971      0.975      0.747\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "      24/30      8.19G     0.7765     0.4039      1.285          5        640: 100% 60/60 [00:36<00:00,  1.63it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 8/8 [00:02<00:00,  3.01it/s]\n",
            "                   all        238        239      0.975      0.989       0.99      0.764\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "      25/30      8.19G     0.7689     0.3961      1.275          5        640: 100% 60/60 [00:36<00:00,  1.64it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 8/8 [00:02<00:00,  2.99it/s]\n",
            "                   all        238        239      0.981      0.963      0.988      0.744\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "      26/30      8.19G     0.7577     0.3785      1.264          5        640: 100% 60/60 [00:36<00:00,  1.65it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 8/8 [00:02<00:00,  3.04it/s]\n",
            "                   all        238        239      0.983      0.988      0.993       0.75\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "      27/30      8.19G     0.7478     0.3647      1.244          5        640: 100% 60/60 [00:36<00:00,  1.64it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 8/8 [00:02<00:00,  3.00it/s]\n",
            "                   all        238        239      0.978      0.994      0.993      0.764\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "      28/30      8.19G     0.7427     0.3581      1.258          5        640: 100% 60/60 [00:36<00:00,  1.62it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 8/8 [00:02<00:00,  3.05it/s]\n",
            "                   all        238        239      0.974      0.985      0.991       0.77\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "      29/30      8.19G     0.7353     0.3489      1.248          5        640: 100% 60/60 [00:36<00:00,  1.64it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 8/8 [00:03<00:00,  2.52it/s]\n",
            "                   all        238        239       0.97      0.992      0.992      0.782\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "      30/30      8.19G     0.7221     0.3408      1.231          5        640: 100% 60/60 [00:36<00:00,  1.63it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 8/8 [00:02<00:00,  3.03it/s]\n",
            "                   all        238        239      0.991      0.982      0.993      0.784\n",
            "\n",
            "30 epochs completed in 0.359 hours.\n",
            "Optimizer stripped from runs/detect/train2/weights/last.pt, 40.5MB\n",
            "Optimizer stripped from runs/detect/train2/weights/best.pt, 40.5MB\n",
            "\n",
            "Validating runs/detect/train2/weights/best.pt...\n",
            "Ultralytics 8.3.88 üöÄ Python-3.11.11 torch-2.5.1+cu124 CUDA:0 (Tesla T4, 15095MiB)\n",
            "YOLO11m summary (fused): 125 layers, 20,032,345 parameters, 0 gradients, 67.7 GFLOPs\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 8/8 [00:03<00:00,  2.09it/s]\n",
            "                   all        238        239      0.988      0.982      0.993      0.782\n",
            "                 Happy        120        121      0.999      0.992      0.995      0.863\n",
            "               Natural         90         90      0.972      0.956      0.988        0.8\n",
            "             Surprised         28         28      0.993          1      0.995      0.684\n",
            "Speed: 0.1ms preprocess, 6.5ms inference, 0.0ms loss, 2.4ms postprocess per image\n",
            "Results saved to \u001b[1mruns/detect/train2\u001b[0m\n",
            "üí° Learn more at https://docs.ultralytics.com/modes/train\n"
          ]
        }
      ],
      "source": [
        "!yolo detect train data=/content/data.yaml model=yolo11m.pt epochs=30 imgsz=640"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vo8BJRXeg0Ap"
      },
      "source": [
        "# Test Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PooP5Vjsg2Jn",
        "outputId": "ebecc9e6-a0a3-4aba-a6db-cc31d879050d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Ultralytics 8.3.88 üöÄ Python-3.11.11 torch-2.5.1+cu124 CUDA:0 (Tesla T4, 15095MiB)\n",
            "YOLO11m summary (fused): 125 layers, 20,032,345 parameters, 0 gradients, 67.7 GFLOPs\n",
            "\n",
            "image 1/238 /content/data/validation/images/01e0ccd3-WIN_20250307_22_07_20_Pro_2.jpg: 384x640 1 Natural, 72.0ms\n",
            "image 2/238 /content/data/validation/images/0226374b-WIN_20250307_22_05_21_Pro.jpg: 384x640 1 Natural, 24.1ms\n",
            "image 3/238 /content/data/validation/images/0240ae34-WIN_20250307_22_25_25_Pro.jpg: 384x640 1 Surprised, 24.0ms\n",
            "image 4/238 /content/data/validation/images/038c453c-WIN_20250307_22_06_06_Pro.jpg: 384x640 1 Natural, 24.0ms\n",
            "image 5/238 /content/data/validation/images/03e8adee-WIN_20250307_22_12_31_Pro.jpg: 384x640 1 Happy, 24.0ms\n",
            "image 6/238 /content/data/validation/images/052bcc02-WIN_20250307_22_29_22_Pro.jpg: 384x640 1 Surprised, 20.1ms\n",
            "image 7/238 /content/data/validation/images/056fb0f4-WIN_20250307_22_11_19_Pro_2.jpg: 384x640 1 Happy, 20.1ms\n",
            "image 8/238 /content/data/validation/images/060230d9-WIN_20250307_22_05_20_Pro_2.jpg: 384x640 1 Natural, 20.1ms\n",
            "image 9/238 /content/data/validation/images/08b6db69-WIN_20250307_22_08_38_Pro.jpg: 384x640 1 Natural, 20.1ms\n",
            "image 10/238 /content/data/validation/images/09af2e0b-WIN_20250307_22_26_09_Pro_2.jpg: 384x640 1 Surprised, 20.1ms\n",
            "image 11/238 /content/data/validation/images/0acd53f1-WIN_20250307_22_11_52_Pro.jpg: 384x640 1 Happy, 20.1ms\n",
            "image 12/238 /content/data/validation/images/0b90de42-WIN_20250307_22_13_41_Pro.jpg: 384x640 1 Happy, 14.5ms\n",
            "image 13/238 /content/data/validation/images/0c8bcad8-WIN_20250307_22_03_59_Pro_2.jpg: 384x640 1 Natural, 14.5ms\n",
            "image 14/238 /content/data/validation/images/0c9b6577-WIN_20250307_22_15_41_Pro_2.jpg: 384x640 1 Happy, 14.6ms\n",
            "image 15/238 /content/data/validation/images/0f1c1ea4-WIN_20250307_22_06_27_Pro_2.jpg: 384x640 1 Natural, 14.6ms\n",
            "image 16/238 /content/data/validation/images/0f549efc-WIN_20250307_22_14_26_Pro_2.jpg: 384x640 1 Happy, 14.3ms\n",
            "image 17/238 /content/data/validation/images/0fc425f0-WIN_20250307_22_05_36_Pro.jpg: 384x640 1 Natural, 14.2ms\n",
            "image 18/238 /content/data/validation/images/10326c65-WIN_20250307_22_11_14_Pro_2.jpg: 384x640 1 Happy, 14.3ms\n",
            "image 19/238 /content/data/validation/images/10806060-WIN_20250307_22_12_44_Pro.jpg: 384x640 1 Happy, 13.2ms\n",
            "image 20/238 /content/data/validation/images/12e1120c-WIN_20250307_22_27_03_Pro_2.jpg: 384x640 1 Surprised, 12.7ms\n",
            "image 21/238 /content/data/validation/images/130e402d-WIN_20250307_22_25_20_Pro_2.jpg: 384x640 1 Surprised, 12.1ms\n",
            "image 22/238 /content/data/validation/images/13192a8b-WIN_20250307_22_13_30_Pro_3.jpg: 384x640 1 Happy, 12.0ms\n",
            "image 23/238 /content/data/validation/images/178babf3-WIN_20250307_22_05_33_Pro.jpg: 384x640 1 Natural, 12.6ms\n",
            "image 24/238 /content/data/validation/images/18056840-WIN_20250307_22_12_25_Pro.jpg: 384x640 1 Happy, 12.4ms\n",
            "image 25/238 /content/data/validation/images/1a909939-WIN_20250307_22_13_50_Pro.jpg: 384x640 1 Happy, 21.2ms\n",
            "image 26/238 /content/data/validation/images/1abb84e4-WIN_20250307_22_06_46_Pro_2.jpg: 384x640 1 Natural, 12.0ms\n",
            "image 27/238 /content/data/validation/images/1ba15dcc-WIN_20250307_22_21_03_Pro.jpg: 384x640 1 Natural, 11.2ms\n",
            "image 28/238 /content/data/validation/images/1dbf42a4-WIN_20250307_22_20_52_Pro_2.jpg: 384x640 1 Natural, 11.8ms\n",
            "image 29/238 /content/data/validation/images/1e2fd851-WIN_20250307_22_28_32_Pro.jpg: 384x640 1 Surprised, 12.5ms\n",
            "image 30/238 /content/data/validation/images/1e31c995-WIN_20250307_22_08_28_Pro.jpg: 384x640 1 Natural, 12.5ms\n",
            "image 31/238 /content/data/validation/images/1f01ed5d-WIN_20250307_22_12_48_Pro_2.jpg: 384x640 1 Happy, 11.3ms\n",
            "image 32/238 /content/data/validation/images/20d3c007-WIN_20250307_22_12_41_Pro.jpg: 384x640 1 Happy, 12.0ms\n",
            "image 33/238 /content/data/validation/images/211601d1-WIN_20250307_22_11_20_Pro_2.jpg: 384x640 1 Happy, 12.8ms\n",
            "image 34/238 /content/data/validation/images/21336a14-WIN_20250307_22_26_49_Pro.jpg: 384x640 1 Surprised, 12.2ms\n",
            "image 35/238 /content/data/validation/images/223a6a47-WIN_20250307_22_12_59_Pro.jpg: 384x640 1 Happy, 12.1ms\n",
            "image 36/238 /content/data/validation/images/22721270-WIN_20250307_22_11_56_Pro.jpg: 384x640 1 Happy, 13.0ms\n",
            "image 37/238 /content/data/validation/images/2338355c-WIN_20250307_22_26_35_Pro.jpg: 384x640 1 Surprised, 12.4ms\n",
            "image 38/238 /content/data/validation/images/245bf13b-WIN_20250307_22_12_49_Pro_2.jpg: 384x640 1 Happy, 12.1ms\n",
            "image 39/238 /content/data/validation/images/274a7769-WIN_20250307_22_13_21_Pro_2.jpg: 384x640 1 Happy, 11.3ms\n",
            "image 40/238 /content/data/validation/images/27d9b231-WIN_20250307_22_25_19_Pro.jpg: 384x640 1 Surprised, 11.5ms\n",
            "image 41/238 /content/data/validation/images/28b65fa6-WIN_20250307_22_17_40_Pro.jpg: 384x640 1 Natural, 12.5ms\n",
            "image 42/238 /content/data/validation/images/2a1bcd88-WIN_20250307_22_04_31_Pro.jpg: 384x640 1 Natural, 12.3ms\n",
            "image 43/238 /content/data/validation/images/2a55bbd8-WIN_20250307_22_23_28_Pro.jpg: 384x640 1 Natural, 11.7ms\n",
            "image 44/238 /content/data/validation/images/2c95743a-WIN_20250307_22_12_12_Pro.jpg: 384x640 1 Happy, 13.1ms\n",
            "image 45/238 /content/data/validation/images/2ccc2692-WIN_20250307_22_12_00_Pro_2.jpg: 384x640 1 Happy, 12.4ms\n",
            "image 46/238 /content/data/validation/images/2ea93b98-WIN_20250307_22_05_30_Pro_2.jpg: 384x640 1 Natural, 11.6ms\n",
            "image 47/238 /content/data/validation/images/2f2ad9be-WIN_20250307_22_13_23_Pro.jpg: 384x640 1 Happy, 11.4ms\n",
            "image 48/238 /content/data/validation/images/2f598947-WIN_20250307_22_12_11_Pro.jpg: 384x640 1 Happy, 12.1ms\n",
            "image 49/238 /content/data/validation/images/303b10b4-WIN_20250307_22_11_50_Pro.jpg: 384x640 1 Happy, 1 Natural, 12.7ms\n",
            "image 50/238 /content/data/validation/images/308b896f-WIN_20250307_22_11_51_Pro_2.jpg: 384x640 1 Happy, 12.0ms\n",
            "image 51/238 /content/data/validation/images/30b55d5b-WIN_20250307_22_17_47_Pro.jpg: 384x640 1 Happy, 11.6ms\n",
            "image 52/238 /content/data/validation/images/31809a47-WIN_20250307_22_06_04_Pro.jpg: 384x640 1 Natural, 11.9ms\n",
            "image 53/238 /content/data/validation/images/32d7d84d-WIN_20250307_22_07_59_Pro.jpg: 384x640 1 Natural, 13.0ms\n",
            "image 54/238 /content/data/validation/images/339a2d49-WIN_20250307_22_27_10_Pro.jpg: 384x640 1 Surprised, 12.0ms\n",
            "image 55/238 /content/data/validation/images/33ca3415-WIN_20250307_22_07_30_Pro_2.jpg: 384x640 1 Natural, 11.4ms\n",
            "image 56/238 /content/data/validation/images/34c2aa7d-WIN_20250307_22_11_24_Pro.jpg: 384x640 1 Happy, 13.7ms\n",
            "image 57/238 /content/data/validation/images/377c4ada-WIN_20250307_22_11_46_Pro.jpg: 384x640 1 Happy, 12.6ms\n",
            "image 58/238 /content/data/validation/images/3821483a-WIN_20250307_22_07_39_Pro_2.jpg: 384x640 1 Natural, 12.0ms\n",
            "image 59/238 /content/data/validation/images/39d94b55-WIN_20250307_22_13_31_Pro_2.jpg: 384x640 1 Happy, 11.8ms\n",
            "image 60/238 /content/data/validation/images/3b2812e7-WIN_20250307_22_15_24_Pro.jpg: 384x640 1 Happy, 12.0ms\n",
            "image 61/238 /content/data/validation/images/3c4afc31-WIN_20250307_22_11_11_Pro_2.jpg: 384x640 1 Happy, 12.5ms\n",
            "image 62/238 /content/data/validation/images/3d70a973-WIN_20250307_22_04_32_Pro.jpg: 384x640 1 Natural, 11.8ms\n",
            "image 63/238 /content/data/validation/images/3e061a27-WIN_20250307_22_12_30_Pro.jpg: 384x640 1 Happy, 11.5ms\n",
            "image 64/238 /content/data/validation/images/3e7f5104-WIN_20250307_22_11_40_Pro_2.jpg: 384x640 1 Happy, 13.7ms\n",
            "image 65/238 /content/data/validation/images/3e889c16-WIN_20250307_22_16_43_Pro.jpg: 384x640 1 Happy, 19.0ms\n",
            "image 66/238 /content/data/validation/images/3eb28174-WIN_20250307_22_17_12_Pro.jpg: 384x640 1 Natural, 12.2ms\n",
            "image 67/238 /content/data/validation/images/422a7fb4-WIN_20250307_22_16_02_Pro.jpg: 384x640 1 Happy, 11.5ms\n",
            "image 68/238 /content/data/validation/images/43ee0b61-WIN_20250307_22_25_31_Pro_2.jpg: 384x640 1 Surprised, 11.4ms\n",
            "image 69/238 /content/data/validation/images/44d9004e-WIN_20250307_22_16_15_Pro.jpg: 384x640 1 Happy, 12.7ms\n",
            "image 70/238 /content/data/validation/images/460598de-WIN_20250307_22_14_29_Pro.jpg: 384x640 1 Happy, 12.4ms\n",
            "image 71/238 /content/data/validation/images/486c0607-WIN_20250307_22_07_33_Pro.jpg: 384x640 1 Natural, 1 Surprised, 12.0ms\n",
            "image 72/238 /content/data/validation/images/49dc0b37-WIN_20250307_22_28_30_Pro.jpg: 384x640 1 Surprised, 11.4ms\n",
            "image 73/238 /content/data/validation/images/4d65009d-WIN_20250307_22_11_42_Pro_2.jpg: 384x640 1 Happy, 12.7ms\n",
            "image 74/238 /content/data/validation/images/4e1c5263-WIN_20250307_22_07_18_Pro.jpg: 384x640 1 Natural, 12.6ms\n",
            "image 75/238 /content/data/validation/images/4e4fa258-WIN_20250307_22_03_45_Pro.jpg: 384x640 1 Natural, 11.6ms\n",
            "image 76/238 /content/data/validation/images/4ee2de25-WIN_20250307_22_15_55_Pro.jpg: 384x640 1 Happy, 11.6ms\n",
            "image 77/238 /content/data/validation/images/511af9ef-WIN_20250307_22_15_41_Pro.jpg: 384x640 1 Happy, 12.3ms\n",
            "image 78/238 /content/data/validation/images/57369adc-WIN_20250307_22_11_24_Pro_2.jpg: 384x640 1 Happy, 12.6ms\n",
            "image 79/238 /content/data/validation/images/5876b84f-WIN_20250307_22_15_05_Pro_2.jpg: 384x640 1 Happy, 13.6ms\n",
            "image 80/238 /content/data/validation/images/5b8a7417-WIN_20250307_22_15_29_Pro_2.jpg: 384x640 1 Happy, 11.7ms\n",
            "image 81/238 /content/data/validation/images/5ba44735-WIN_20250307_22_29_35_Pro_2.jpg: 384x640 1 Surprised, 12.3ms\n",
            "image 82/238 /content/data/validation/images/5c6d5b58-WIN_20250307_22_06_05_Pro.jpg: 384x640 1 Natural, 11.5ms\n",
            "image 83/238 /content/data/validation/images/5dcd4d41-WIN_20250307_22_08_23_Pro.jpg: 384x640 1 Natural, 15.9ms\n",
            "image 84/238 /content/data/validation/images/5f5c8867-WIN_20250307_22_08_50_Pro.jpg: 384x640 1 Natural, 12.1ms\n",
            "image 85/238 /content/data/validation/images/5f873126-WIN_20250307_22_27_02_Pro.jpg: 384x640 1 Surprised, 11.5ms\n",
            "image 86/238 /content/data/validation/images/635faf22-WIN_20250307_22_12_43_Pro.jpg: 384x640 1 Happy, 11.8ms\n",
            "image 87/238 /content/data/validation/images/64af1dad-WIN_20250307_22_28_29_Pro_2.jpg: 384x640 1 Surprised, 12.3ms\n",
            "image 88/238 /content/data/validation/images/64c10022-WIN_20250307_22_16_54_Pro.jpg: 384x640 1 Happy, 12.5ms\n",
            "image 89/238 /content/data/validation/images/65ba08e9-WIN_20250307_22_12_22_Pro.jpg: 384x640 1 Happy, 11.8ms\n",
            "image 90/238 /content/data/validation/images/6654f11f-WIN_20250307_22_16_51_Pro.jpg: 384x640 1 Happy, 11.4ms\n",
            "image 91/238 /content/data/validation/images/68753cfc-WIN_20250307_22_19_31_Pro.jpg: 384x640 1 Happy, 1 Natural, 12.3ms\n",
            "image 92/238 /content/data/validation/images/695f2653-WIN_20250307_22_08_04_Pro_2.jpg: 384x640 1 Natural, 12.6ms\n",
            "image 93/238 /content/data/validation/images/6e3ad106-WIN_20250307_22_12_15_Pro.jpg: 384x640 1 Happy, 11.6ms\n",
            "image 94/238 /content/data/validation/images/7101bced-WIN_20250307_22_14_51_Pro.jpg: 384x640 1 Happy, 11.5ms\n",
            "image 95/238 /content/data/validation/images/73056a09-WIN_20250307_22_22_49_Pro.jpg: 384x640 1 Natural, 12.4ms\n",
            "image 96/238 /content/data/validation/images/73fe622f-WIN_20250307_22_12_47_Pro_2.jpg: 384x640 1 Happy, 12.4ms\n",
            "image 97/238 /content/data/validation/images/74367673-WIN_20250307_22_05_44_Pro_2.jpg: 384x640 1 Natural, 11.5ms\n",
            "image 98/238 /content/data/validation/images/75a4b77e-WIN_20250307_22_21_56_Pro.jpg: 384x640 1 Natural, 11.5ms\n",
            "image 99/238 /content/data/validation/images/75fd05cf-WIN_20250307_22_26_25_Pro_2.jpg: 384x640 1 Surprised, 12.7ms\n",
            "image 100/238 /content/data/validation/images/765b629b-WIN_20250307_22_11_41_Pro_3.jpg: 384x640 1 Happy, 11.8ms\n",
            "image 101/238 /content/data/validation/images/77570203-WIN_20250307_22_06_41_Pro.jpg: 384x640 1 Natural, 11.7ms\n",
            "image 102/238 /content/data/validation/images/77c8ee0a-WIN_20250307_22_09_12_Pro_3.jpg: 384x640 1 Natural, 12.5ms\n",
            "image 103/238 /content/data/validation/images/78f7b11b-WIN_20250307_22_16_47_Pro.jpg: 384x640 1 Happy, 14.7ms\n",
            "image 104/238 /content/data/validation/images/7bdcd775-WIN_20250307_22_16_08_Pro.jpg: 384x640 1 Happy, 17.4ms\n",
            "image 105/238 /content/data/validation/images/7e92ffb9-WIN_20250307_22_14_38_Pro.jpg: 384x640 1 Happy, 12.6ms\n",
            "image 106/238 /content/data/validation/images/7fc9cc7a-WIN_20250307_22_11_07_Pro.jpg: 384x640 1 Happy, 14.3ms\n",
            "image 107/238 /content/data/validation/images/80098d64-WIN_20250307_22_08_53_Pro_3.jpg: 384x640 1 Natural, 12.7ms\n",
            "image 108/238 /content/data/validation/images/8100cad4-WIN_20250307_22_06_44_Pro_2.jpg: 384x640 1 Natural, 11.8ms\n",
            "image 109/238 /content/data/validation/images/82a69758-WIN_20250307_22_06_52_Pro.jpg: 384x640 1 Natural, 11.7ms\n",
            "image 110/238 /content/data/validation/images/83a2c170-WIN_20250307_22_16_11_Pro.jpg: 384x640 1 Happy, 12.6ms\n",
            "image 111/238 /content/data/validation/images/83ac479c-WIN_20250307_22_11_22_Pro.jpg: 384x640 1 Happy, 11.9ms\n",
            "image 112/238 /content/data/validation/images/84dd1739-WIN_20250307_22_12_55_Pro.jpg: 384x640 1 Happy, 12.9ms\n",
            "image 113/238 /content/data/validation/images/858bc22f-WIN_20250307_22_26_53_Pro.jpg: 384x640 1 Surprised, 12.1ms\n",
            "image 114/238 /content/data/validation/images/86771a80-WIN_20250307_22_05_40_Pro_2.jpg: 384x640 1 Natural, 11.9ms\n",
            "image 115/238 /content/data/validation/images/896886a9-WIN_20250307_22_06_52_Pro_2.jpg: 384x640 1 Natural, 11.5ms\n",
            "image 116/238 /content/data/validation/images/89df4b19-WIN_20250307_22_15_23_Pro.jpg: 384x640 1 Happy, 11.8ms\n",
            "image 117/238 /content/data/validation/images/8e231d68-WIN_20250307_22_07_19_Pro.jpg: 384x640 1 Natural, 13.1ms\n",
            "image 118/238 /content/data/validation/images/8e4daafa-WIN_20250307_22_16_07_Pro.jpg: 384x640 1 Happy, 12.0ms\n",
            "image 119/238 /content/data/validation/images/8e9c43a9-WIN_20250307_22_06_50_Pro_2.jpg: 384x640 1 Natural, 11.4ms\n",
            "image 120/238 /content/data/validation/images/8f13b176-WIN_20250307_22_12_40_Pro_2.jpg: 384x640 1 Happy, 11.7ms\n",
            "image 121/238 /content/data/validation/images/90b5addc-WIN_20250307_22_17_48_Pro.jpg: 384x640 1 Happy, 13.5ms\n",
            "image 122/238 /content/data/validation/images/91be9aa6-WIN_20250307_22_05_44_Pro.jpg: 384x640 1 Natural, 12.5ms\n",
            "image 123/238 /content/data/validation/images/94591ce0-WIN_20250307_22_08_39_Pro.jpg: 384x640 1 Natural, 11.5ms\n",
            "image 124/238 /content/data/validation/images/945bc4a4-WIN_20250307_22_16_30_Pro.jpg: 384x640 1 Happy, 11.5ms\n",
            "image 125/238 /content/data/validation/images/97ed2124-WIN_20250307_22_06_54_Pro_2.jpg: 384x640 1 Natural, 12.3ms\n",
            "image 126/238 /content/data/validation/images/9b3fc23f-WIN_20250307_22_26_08_Pro_2.jpg: 384x640 1 Surprised, 12.4ms\n",
            "image 127/238 /content/data/validation/images/9befec4d-WIN_20250307_22_12_14_Pro.jpg: 384x640 1 Happy, 11.5ms\n",
            "image 128/238 /content/data/validation/images/9c01656e-WIN_20250307_22_11_49_Pro_3.jpg: 384x640 1 Happy, 1 Natural, 11.5ms\n",
            "image 129/238 /content/data/validation/images/9c1d822d-WIN_20250307_22_08_10_Pro.jpg: 384x640 1 Natural, 12.2ms\n",
            "image 130/238 /content/data/validation/images/9c1fd37f-WIN_20250307_22_03_41_Pro.jpg: 384x640 1 Natural, 12.9ms\n",
            "image 131/238 /content/data/validation/images/9ceb8420-WIN_20250307_22_07_48_Pro.jpg: 384x640 1 Natural, 12.1ms\n",
            "image 132/238 /content/data/validation/images/9d6e0e61-WIN_20250307_22_12_36_Pro_2.jpg: 384x640 1 Happy, 11.4ms\n",
            "image 133/238 /content/data/validation/images/9dff27b0-WIN_20250307_22_15_15_Pro_2.jpg: 384x640 1 Happy, 11.8ms\n",
            "image 134/238 /content/data/validation/images/9ec18a65-WIN_20250307_22_06_50_Pro.jpg: 384x640 1 Natural, 12.8ms\n",
            "image 135/238 /content/data/validation/images/9f6f025d-WIN_20250307_22_11_21_Pro_2.jpg: 384x640 1 Happy, 12.3ms\n",
            "image 136/238 /content/data/validation/images/9fddeb52-WIN_20250307_22_13_19_Pro_2.jpg: 384x640 1 Happy, 11.5ms\n",
            "image 137/238 /content/data/validation/images/a0698c78-WIN_20250307_22_05_10_Pro.jpg: 384x640 1 Natural, 11.7ms\n",
            "image 138/238 /content/data/validation/images/a07967a1-WIN_20250307_22_05_19_Pro.jpg: 384x640 1 Natural, 13.0ms\n",
            "image 139/238 /content/data/validation/images/a095489b-WIN_20250307_22_04_33_Pro.jpg: 384x640 1 Natural, 12.3ms\n",
            "image 140/238 /content/data/validation/images/a1789582-WIN_20250307_22_13_24_Pro_2.jpg: 384x640 1 Happy, 11.5ms\n",
            "image 141/238 /content/data/validation/images/a2ea0894-WIN_20250307_22_12_33_Pro_2.jpg: 384x640 1 Happy, 11.7ms\n",
            "image 142/238 /content/data/validation/images/a327f4a0-WIN_20250307_22_17_04_Pro.jpg: 384x640 1 Happy, 13.7ms\n",
            "image 143/238 /content/data/validation/images/a3417f60-WIN_20250307_22_27_54_Pro_2.jpg: 384x640 1 Surprised, 12.2ms\n",
            "image 144/238 /content/data/validation/images/a3abcd34-WIN_20250307_22_08_00_Pro_2.jpg: 384x640 1 Natural, 19.4ms\n",
            "image 145/238 /content/data/validation/images/a494b571-WIN_20250307_22_23_25_Pro.jpg: 384x640 1 Natural, 11.8ms\n",
            "image 146/238 /content/data/validation/images/a6557cb5-WIN_20250307_22_24_02_Pro.jpg: 384x640 1 Natural, 12.4ms\n",
            "image 147/238 /content/data/validation/images/a66c9a6d-WIN_20250307_22_19_35_Pro.jpg: 384x640 1 Happy, 11.5ms\n",
            "image 148/238 /content/data/validation/images/a698ccc5-WIN_20250307_22_12_31_Pro_2.jpg: 384x640 1 Happy, 12.8ms\n",
            "image 149/238 /content/data/validation/images/a7418dfd-WIN_20250307_22_08_15_Pro_2.jpg: 384x640 1 Natural, 12.4ms\n",
            "image 150/238 /content/data/validation/images/a7b1521e-WIN_20250307_22_11_35_Pro_2.jpg: 384x640 1 Happy, 11.4ms\n",
            "image 151/238 /content/data/validation/images/a896e3d0-WIN_20250307_22_11_25_Pro.jpg: 384x640 1 Happy, 12.4ms\n",
            "image 152/238 /content/data/validation/images/a919a89c-WIN_20250307_22_26_10_Pro.jpg: 384x640 1 Surprised, 13.0ms\n",
            "image 153/238 /content/data/validation/images/a965fd99-WIN_20250307_22_08_18_Pro.jpg: 384x640 1 Natural, 13.8ms\n",
            "image 154/238 /content/data/validation/images/aac77290-WIN_20250307_22_07_16_Pro_2.jpg: 384x640 1 Natural, 11.5ms\n",
            "image 155/238 /content/data/validation/images/abce5fba-WIN_20250307_22_15_09_Pro.jpg: 384x640 1 Happy, 12.0ms\n",
            "image 156/238 /content/data/validation/images/ac1d4bb7-WIN_20250307_22_09_13_Pro_2.jpg: 384x640 1 Natural, 12.9ms\n",
            "image 157/238 /content/data/validation/images/adf89a73-WIN_20250307_22_13_32_Pro_2.jpg: 384x640 1 Happy, 12.2ms\n",
            "image 158/238 /content/data/validation/images/ae21e88b-WIN_20250307_22_21_05_Pro.jpg: 384x640 1 Natural, 11.4ms\n",
            "image 159/238 /content/data/validation/images/afe3e773-WIN_20250307_22_28_06_Pro.jpg: 384x640 1 Surprised, 11.8ms\n",
            "image 160/238 /content/data/validation/images/b21a9674-WIN_20250307_22_07_38_Pro_2.jpg: 384x640 1 Natural, 13.1ms\n",
            "image 161/238 /content/data/validation/images/b492bd94-WIN_20250307_22_18_12_Pro.jpg: 384x640 1 Happy, 13.2ms\n",
            "image 162/238 /content/data/validation/images/b7faba0d-WIN_20250307_22_11_14_Pro.jpg: 384x640 1 Happy, 11.6ms\n",
            "image 163/238 /content/data/validation/images/b9cc6744-WIN_20250307_22_04_13_Pro.jpg: 384x640 1 Natural, 11.7ms\n",
            "image 164/238 /content/data/validation/images/ba1e08b0-WIN_20250307_22_06_28_Pro.jpg: 384x640 1 Natural, 12.7ms\n",
            "image 165/238 /content/data/validation/images/bb612c88-WIN_20250307_22_07_52_Pro.jpg: 384x640 1 Natural, 12.5ms\n",
            "image 166/238 /content/data/validation/images/bbb27453-WIN_20250307_22_07_58_Pro_2.jpg: 384x640 1 Natural, 11.7ms\n",
            "image 167/238 /content/data/validation/images/bbf44a9a-WIN_20250307_22_12_52_Pro.jpg: 384x640 1 Happy, 11.7ms\n",
            "image 168/238 /content/data/validation/images/bc02fd9a-WIN_20250307_22_15_01_Pro_2.jpg: 384x640 1 Happy, 13.0ms\n",
            "image 169/238 /content/data/validation/images/bf7e5586-WIN_20250307_22_13_40_Pro_3.jpg: 384x640 1 Happy, 12.9ms\n",
            "image 170/238 /content/data/validation/images/bfc63b33-WIN_20250307_22_13_15_Pro_3.jpg: 384x640 1 Happy, 11.7ms\n",
            "image 171/238 /content/data/validation/images/c115d5df-WIN_20250307_22_20_53_Pro.jpg: 384x640 1 Natural, 11.5ms\n",
            "image 172/238 /content/data/validation/images/c1c4423c-WIN_20250307_22_22_57_Pro.jpg: 384x640 1 Natural, 12.2ms\n",
            "image 173/238 /content/data/validation/images/c1d78419-WIN_20250307_22_14_38_Pro_2.jpg: 384x640 1 Happy, 13.1ms\n",
            "image 174/238 /content/data/validation/images/c2066436-WIN_20250307_22_06_57_Pro.jpg: 384x640 1 Natural, 11.9ms\n",
            "image 175/238 /content/data/validation/images/c218bfdc-WIN_20250307_22_14_54_Pro_2.jpg: 384x640 1 Happy, 11.6ms\n",
            "image 176/238 /content/data/validation/images/c28a3c09-WIN_20250307_22_27_54_Pro.jpg: 384x640 1 Surprised, 12.2ms\n",
            "image 177/238 /content/data/validation/images/c30ba289-WIN_20250307_22_14_45_Pro.jpg: 384x640 1 Happy, 13.0ms\n",
            "image 178/238 /content/data/validation/images/c3146cee-WIN_20250307_22_08_49_Pro.jpg: 384x640 1 Natural, 11.7ms\n",
            "image 179/238 /content/data/validation/images/c426af55-WIN_20250307_22_11_15_Pro.jpg: 384x640 1 Happy, 11.5ms\n",
            "image 180/238 /content/data/validation/images/c5e2afe8-WIN_20250307_22_11_59_Pro_2.jpg: 384x640 1 Happy, 12.2ms\n",
            "image 181/238 /content/data/validation/images/c9254757-WIN_20250307_22_15_22_Pro.jpg: 384x640 1 Happy, 13.7ms\n",
            "image 182/238 /content/data/validation/images/cb2ff71e-WIN_20250307_22_04_37_Pro.jpg: 384x640 1 Natural, 11.6ms\n",
            "image 183/238 /content/data/validation/images/cb3212c5-WIN_20250307_22_15_43_Pro.jpg: 384x640 1 Happy, 11.5ms\n",
            "image 184/238 /content/data/validation/images/cc03e226-WIN_20250307_22_18_41_Pro_2.jpg: 384x640 1 Happy, 13.9ms\n",
            "image 185/238 /content/data/validation/images/cc175db7-WIN_20250307_22_11_17_Pro_2.jpg: 384x640 1 Happy, 12.2ms\n",
            "image 186/238 /content/data/validation/images/cc268bf9-WIN_20250307_22_12_07_Pro_2.jpg: 384x640 1 Happy, 11.7ms\n",
            "image 187/238 /content/data/validation/images/cca7b234-WIN_20250307_22_17_37_Pro.jpg: 384x640 1 Happy, 12.8ms\n",
            "image 188/238 /content/data/validation/images/cdf46cb3-WIN_20250307_22_05_18_Pro_2.jpg: 384x640 1 Natural, 12.3ms\n",
            "image 189/238 /content/data/validation/images/cffb2011-WIN_20250307_22_15_57_Pro.jpg: 384x640 1 Happy, 11.6ms\n",
            "image 190/238 /content/data/validation/images/d3045cf8-WIN_20250307_22_08_23_Pro_2.jpg: 384x640 1 Natural, 11.8ms\n",
            "image 191/238 /content/data/validation/images/d311aa46-WIN_20250307_22_17_39_Pro.jpg: 384x640 1 Natural, 12.4ms\n",
            "image 192/238 /content/data/validation/images/d4acd3df-WIN_20250307_22_07_54_Pro_3.jpg: 384x640 1 Natural, 12.2ms\n",
            "image 193/238 /content/data/validation/images/d4d14cb7-WIN_20250307_22_12_13_Pro.jpg: 384x640 1 Happy, 13.2ms\n",
            "image 194/238 /content/data/validation/images/d4e2f0e9-WIN_20250307_22_21_55_Pro_2.jpg: 384x640 1 Natural, 11.4ms\n",
            "image 195/238 /content/data/validation/images/d559862b-WIN_20250307_22_16_04_Pro_2.jpg: 384x640 1 Happy, 12.3ms\n",
            "image 196/238 /content/data/validation/images/d57f97ab-WIN_20250307_22_25_21_Pro.jpg: 384x640 1 Surprised, 13.0ms\n",
            "image 197/238 /content/data/validation/images/d5ad393f-WIN_20250307_22_08_31_Pro_3.jpg: 384x640 1 Natural, 12.3ms\n",
            "image 198/238 /content/data/validation/images/d62f54d2-WIN_20250307_22_18_49_Pro_2.jpg: 384x640 1 Happy, 11.4ms\n",
            "image 199/238 /content/data/validation/images/d8378762-WIN_20250307_22_26_26_Pro.jpg: 384x640 1 Surprised, 11.6ms\n",
            "image 200/238 /content/data/validation/images/da60ad23-WIN_20250307_22_07_49_Pro_2.jpg: 384x640 1 Natural, 13.8ms\n",
            "image 201/238 /content/data/validation/images/da766c82-WIN_20250307_22_23_07_Pro.jpg: 384x640 1 Natural, 12.9ms\n",
            "image 202/238 /content/data/validation/images/daf43fb6-WIN_20250307_22_08_44_Pro_2.jpg: 384x640 1 Natural, 11.7ms\n",
            "image 203/238 /content/data/validation/images/dbf017ac-WIN_20250307_22_12_44_Pro_2.jpg: 384x640 1 Happy, 11.5ms\n",
            "image 204/238 /content/data/validation/images/dcaf5f88-WIN_20250307_22_26_15_Pro_2.jpg: 384x640 1 Surprised, 12.2ms\n",
            "image 205/238 /content/data/validation/images/dcc1b1ee-WIN_20250307_22_15_04_Pro_2.jpg: 384x640 1 Happy, 13.4ms\n",
            "image 206/238 /content/data/validation/images/dd88e375-WIN_20250307_22_15_00_Pro.jpg: 384x640 1 Happy, 11.5ms\n",
            "image 207/238 /content/data/validation/images/dd8accc5-WIN_20250307_22_06_47_Pro_3.jpg: 384x640 1 Natural, 11.5ms\n",
            "image 208/238 /content/data/validation/images/de263d04-WIN_20250307_22_07_37_Pro.jpg: 384x640 1 Natural, 12.4ms\n",
            "image 209/238 /content/data/validation/images/de81339b-WIN_20250307_22_16_23_Pro.jpg: 384x640 1 Happy, 13.0ms\n",
            "image 210/238 /content/data/validation/images/e1dc9de3-WIN_20250307_22_09_10_Pro.jpg: 384x640 1 Natural, 12.0ms\n",
            "image 211/238 /content/data/validation/images/e4d4c7ec-WIN_20250307_22_18_18_Pro.jpg: 384x640 1 Happy, 11.4ms\n",
            "image 212/238 /content/data/validation/images/e6e55de0-WIN_20250307_22_23_00_Pro.jpg: 384x640 1 Natural, 11.8ms\n",
            "image 213/238 /content/data/validation/images/e72e47cb-WIN_20250307_22_13_47_Pro.jpg: 384x640 1 Happy, 12.7ms\n",
            "image 214/238 /content/data/validation/images/e7813535-WIN_20250307_22_15_10_Pro.jpg: 384x640 1 Happy, 12.8ms\n",
            "image 215/238 /content/data/validation/images/e845a43e-WIN_20250307_22_12_47_Pro.jpg: 384x640 1 Happy, 11.6ms\n",
            "image 216/238 /content/data/validation/images/e96802f6-WIN_20250307_22_12_26_Pro_2.jpg: 384x640 1 Happy, 11.6ms\n",
            "image 217/238 /content/data/validation/images/ecd078c3-WIN_20250307_22_15_14_Pro_2.jpg: 384x640 1 Happy, 13.7ms\n",
            "image 218/238 /content/data/validation/images/ee6c460d-WIN_20250307_22_05_31_Pro_2.jpg: 384x640 1 Natural, 12.2ms\n",
            "image 219/238 /content/data/validation/images/ef066463-WIN_20250307_22_08_59_Pro.jpg: 384x640 1 Natural, 11.4ms\n",
            "image 220/238 /content/data/validation/images/ef6c3873-WIN_20250307_22_12_20_Pro.jpg: 384x640 1 Happy, 11.9ms\n",
            "image 221/238 /content/data/validation/images/f1629322-WIN_20250307_22_18_19_Pro.jpg: 384x640 1 Happy, 14.0ms\n",
            "image 222/238 /content/data/validation/images/f292b119-WIN_20250307_22_22_41_Pro.jpg: 384x640 1 Natural, 12.1ms\n",
            "image 223/238 /content/data/validation/images/f48a37da-WIN_20250307_22_05_33_Pro_2.jpg: 384x640 1 Natural, 11.5ms\n",
            "image 224/238 /content/data/validation/images/f4c697a8-WIN_20250307_22_26_06_Pro.jpg: 384x640 1 Natural, 12.1ms\n",
            "image 225/238 /content/data/validation/images/f4cba733-WIN_20250307_22_16_45_Pro.jpg: 384x640 1 Happy, 1 Natural, 18.8ms\n",
            "image 226/238 /content/data/validation/images/f7571c6c-WIN_20250307_22_19_35_Pro_2.jpg: 384x640 1 Happy, 11.8ms\n",
            "image 227/238 /content/data/validation/images/f86e0b2c-WIN_20250307_22_06_54_Pro.jpg: 384x640 1 Natural, 12.5ms\n",
            "image 228/238 /content/data/validation/images/f94b229a-WIN_20250307_22_09_09_Pro_2.jpg: 384x640 1 Natural, 11.8ms\n",
            "image 229/238 /content/data/validation/images/fba9c821-WIN_20250307_22_08_02_Pro.jpg: 384x640 1 Natural, 11.4ms\n",
            "image 230/238 /content/data/validation/images/fbacee95-WIN_20250307_22_25_23_Pro.jpg: 384x640 1 Surprised, 11.8ms\n",
            "image 231/238 /content/data/validation/images/fbb23134-WIN_20250307_22_21_02_Pro.jpg: 384x640 1 Natural, 13.3ms\n",
            "image 232/238 /content/data/validation/images/fbcd2d50-WIN_20250307_22_07_31_Pro_2.jpg: 384x640 1 Natural, 12.6ms\n",
            "image 233/238 /content/data/validation/images/fc44280a-WIN_20250307_22_11_08_Pro.jpg: 384x640 1 Happy, 12.4ms\n",
            "image 234/238 /content/data/validation/images/fc5bf29d-WIN_20250307_22_11_23_Pro_2.jpg: 384x640 1 Happy, 11.6ms\n",
            "image 235/238 /content/data/validation/images/fcc56156-WIN_20250307_22_27_04_Pro.jpg: 384x640 1 Surprised, 12.6ms\n",
            "image 236/238 /content/data/validation/images/fd38f040-WIN_20250307_22_09_06_Pro.jpg: 384x640 1 Natural, 12.9ms\n",
            "image 237/238 /content/data/validation/images/fd5740b2-WIN_20250307_22_13_29_Pro_2.jpg: 384x640 1 Happy, 12.1ms\n",
            "image 238/238 /content/data/validation/images/fed49f8f-WIN_20250307_22_15_08_Pro.jpg: 384x640 1 Happy, 1 Surprised, 11.4ms\n",
            "Speed: 1.8ms preprocess, 13.1ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Results saved to \u001b[1mruns/detect/predict2\u001b[0m\n",
            "üí° Learn more at https://docs.ultralytics.com/modes/predict\n"
          ]
        }
      ],
      "source": [
        "!yolo detect predict model=runs/detect/train/weights/best.pt source=data/validation/images save=True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zEEObQqoiGrs"
      },
      "outputs": [],
      "source": [
        "import glob\n",
        "from IPython.display import Image, display\n",
        "for image_path in glob.glob(f'/content/runs/detect/predict/*.jpg')[:5]:\n",
        "  display(Image(filename=image_path, height=400))\n",
        "  print('\\n')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IcoBAeHXa86W"
      },
      "source": [
        "# Download Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qcBdnOA9v85S",
        "outputId": "70709059-71f2-44d8-b024-a4f5dd15141b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/my_model\n",
            "  adding: my_model.pt (deflated 38%)\n",
            "  adding: train/ (stored 0%)\n",
            "  adding: train/results.csv (deflated 59%)\n",
            "  adding: train/labels_correlogram.jpg (deflated 44%)\n",
            "  adding: train/args.yaml (deflated 53%)\n",
            "  adding: train/weights/ (stored 0%)\n",
            "  adding: train/weights/last.pt (deflated 38%)\n",
            "  adding: train/weights/best.pt (deflated 38%)\n",
            "  adding: train/labels.jpg (deflated 39%)\n",
            "  adding: train/events.out.tfevents.1741785176.cb4357930276.777.0 (deflated 93%)\n",
            "  adding: train/train_batch1.jpg (deflated 7%)\n",
            "  adding: train/train_batch2.jpg (deflated 8%)\n",
            "  adding: train/train_batch0.jpg (deflated 10%)\n",
            "/content\n"
          ]
        }
      ],
      "source": [
        "# Create \"my_model\" folder to store model weights and train results\n",
        "!mkdir /content/my_model\n",
        "!cp /content/runs/detect/train/weights/best.pt /content/my_model/my_model.pt\n",
        "!cp -r /content/runs/detect/train /content/my_model\n",
        "\n",
        "%cd my_model\n",
        "!zip /content/my_model.zip my_model.pt\n",
        "!zip -r /content/my_model.zip train\n",
        "%cd /content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "43ypwonynLVu"
      },
      "outputs": [],
      "source": [
        "from google.colab import files\n",
        "\n",
        "files.download('/content/my_model.zip')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YL06c6pb_UqZ"
      },
      "source": [
        "## Deploy YOLO Model on PC\n",
        "\n",
        "Next, we'll take our downloaded model and run it on a local device. This section provides instructions showing how to deploy YOLO models on various devices."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gzaJQ2sGEPhP"
      },
      "source": [
        "The easiest way to run Ultralytics YOLO models on a PC is using **Anaconda**, which helps set up a virtual Python environment with all necessary dependencies.\n",
        "\n",
        "### **1. Download and Install Anaconda**\n",
        "[Download Anaconda](https://anaconda.com/download) and follow the default installation settings.\n",
        "\n",
        "### **2. Set up a virtual environment**\n",
        "Open **Anaconda Prompt** (or a terminal on macOS/Linux) and create a new Python environment:\n",
        "```\n",
        "conda create --name yolo-env python=3.12 -y\n",
        "conda activate yolo-env\n",
        "```\n",
        "\n",
        "Then, install Ultralytics:\n",
        "\n",
        "```\n",
        "pip install ultralytics\n",
        "```\n",
        "\n",
        "For NVIDIA GPU users, install the GPU-accelerated version of PyTorch:\n",
        "\n",
        "```\n",
        "pip install --upgrade torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu124\n",
        "```\n",
        "\n",
        "### 3. Extract the trained model\n",
        "Unzip ```my_model.zip``` and navigate into its folder:\n",
        "```\n",
        "cd path/to/folder\n",
        "```\n",
        "#### 3.1. Download my model\n",
        "If you want to, you may download my face expressions trained model and run it.\n",
        "\n",
        "Simply click ```my_model.pt``` and select 'Download raw file'.\n",
        "\n",
        "### 4. Download and run yolo_detect.py\n",
        "Download the detection script:\n",
        "\n",
        "```\n",
        "curl -o yolo_detect.py https://raw.githubusercontent.com/Etaizil/Train_Yolo_Model/refs/heads/main/yolo_detect.py\n",
        "```\n",
        "\n",
        "Run inference with a YOLOv8n model on a laptop or USB camera at 1280x720 resolution:\n",
        "\n",
        "```\n",
        "python yolo_detect.py --model my_model.pt --source laptop0 --resolution 720x720\n",
        "```\n",
        "\n",
        "This will open a window displaying a live feed with bounding boxes drawn around detected facial expressions.\n",
        "\n",
        "Alternatively, to process an image, video, or folder of images, specify the source:\n",
        "\n",
        "```\n",
        "python yolo_detect.py --model my_model.pt --source path/to/image_or_video.mp4\n",
        "```\n",
        "\n",
        "You can also run the model on an video file. For full instructions see the [README file](https://github.com/Etaizil/Train_Yolo_Model/blob/main/README.md).\n",
        "\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
